# Запуск локального сервера

Минимальные требования к серверу для локального развертывания LLM:

- Сервер или персональный компьютер на базе серверного процессова (Intel Xeon и т.п.), поддерживающего многопоточность;
- Не менее 32 Гб ОЗУ;
- Средства ускорения параллельных вычислений (графический ускоритель), не менее NVidia 3060 RTX 12 Гб ОЗУ (рекомендуется от NVidia 4080 24 Гб и выше);
- Операционная система Linux Ubuntu 24.04 ;
- Актуальные драйвера CUDA и средства разработки CUDNN;

Клонируйте репозиторий:
```
$ https://github.com/YaRoLit/XAKATOH_S21.git
```
Установите необходимые для работы приложения библиотеки python:
```
$ cd /src/llm_reocessing && pip install -r requirements.txt 
```
Скачайте большую яыковую модель, поддерживающую llama-cpp, например:
```
https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF
```
Укажите путь к модели в скрипте llm_server.py. Запустите приложение:
```
$ python3 llm_server.py
```